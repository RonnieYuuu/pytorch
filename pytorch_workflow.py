# -*- coding: utf-8 -*-
"""pytorch_workflow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-cwrtVj2lLGJ2djf3GDR9zEEUfUv0gwt
"""

import torch
from torch import nn
torch.__version__

"""**Creating a simple dataset using the linear regression formula**"""

weight = 0.7
bias = 0.3

start = 0
end = 1
step = 0.02

X = torch.arange(start, end, step).unsqueeze(dim=1)
y = weight * X + bias

X[:10], y[:10], len(X), len(y)

"""**Splitting our data into training and test sets**"""

train_split = int(0.8 * len(X))
train_split

X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test = X[train_split:], y[train_split:]
len(X_train), len(y_train), len(X_test), len(y_test)

"""**Building a function to visualize our data**"""

import matplotlib.pyplot as plt

def plot_predictions(train_data = X_train,
                     train_labels = y_train,
                     test_data = X_test,
                     test_labels = y_test,
                     predictions = None):
  """
  plots training data, test data and compares predictions
  """


  plt.figure(figsize = (10, 7))

  # Plot training data in blue
  plt.scatter(train_data, train_labels, c="b", s=4, label="Training data")

  # Plot test data in green
  plt.scatter(test_data, test_labels, c="g", s=4, label="Testing data")

  # Are there predictions?
  if predictions is not None:
    #plot the predictions if they exist
    plt.scatter(test_data, predictions, c="r", s=4, label = "Predictions")

  # show the legend
  plt.legend(prop = {"size":14})

plot_predictions()

"""**Building Model**"""

class LinearRegressionModel(nn.Module):
  def __init__(self):
    super().__init__()
    self.weights = nn.Parameter(torch.randn(1,
                                            requires_grad=True,
                                            dtype=float))
    self.bias = nn.Parameter(torch.randn(1,
                                         requires_grad=True,
                                         dtype=float))
  # Forward method to define the computation in the model
  def forward(self, x: torch.Tensor) -> torch.Tensor:
    return self.weights * x + self.bias

"""**Checking the contents of our pytorch model**"""

# .parameters() -- see what's inside the model

# Create a random seed
torch.manual_seed(42)

# Create an instance of the model
Model_0 = LinearRegressionModel()

Model_0

# Check out the parameters
list(Model_0.parameters())

# List named parameters

Model_0.state_dict()

weight, bias

# Making prediction using 'torch.inference_mode()' -- To check the model prediction power
# When we pass data through the model, it's going to run through the forward() method

X_test, y_test

with torch.inference_mode():
  y_preds = Model_0(X_test)

y_preds

plot_predictions(predictions = y_preds)

# 直接用，gradient tracking
y_preds = Model_0(X_test)
y_preds

"""**Loss Function (cost function): Measure how wrong the model's prediction are to the ideal outputss, the lower, the better.

**Optimizer: Take into account the loss of a model and adjusts the models parameters to improve the loss function.


"""

# Set up loss function
loss_fn = nn.L1Loss()

# Set an optimizer (SGD)
optimizer = torch.optim.SGD(params=Model_0.parameters(), lr = 0.01) # learning rate (the smaller, the smaller the change in parameters)

"""# Buliding a training loop in pytorch
0. Loop through the data
1. Forward pass (this involves data moving through our model's forward() functions.) to make predictions on data -- also called Forward Propagation
2. Calculate the loss -- Compare forward pass predictions to ground truth loss
3. Optimizer zero grad
4. Loss backward -- move backwards through the network to calculate the gradients of each of the parameters of our model with respect to loss. (backpropagation)
5. Optimizer step -- use the optimizer to try and improve the loss. (gradient descent)

"""

torch.manual_seed(42)

# A epoch is one loop through the data.
epochs = 200

epoch_count = []
loss_values = []
test_loss_values = []

# Loop through the data
for epoch in range(epochs):
  # Set the model to training mode
  Model_0.train()

  # Forward Pass
  y_pred = Model_0(X_train)

  # Calculate the loss
  loss = loss_fn(y_pred, y_train)
  print(f"Loss at epoch {epochs}: {loss}")

  # Optimizer zero grad
  optimizer.zero_grad()

  # Perform backpropagetion on loss with respect to the parameters of the model
  loss.backward()

  # Step the optimizer (Perform Gradient Descent)
  optimizer.step()
  # By default how the optimizer changes will acculumate through the loop, so we have to zero them above in step 3. for the next interation of the loop (Optimizer zero grad )

  # Testing
  Model_0.eval() # Turns off gradient tracking
  with torch.inference_mode(): #turns off gradient tracking
      # 1. Do the forward pass
      test_pred = Model_0(X_test)

      # 2. Caculate the loss
      test_loss = loss_fn(test_pred, y_test)

  if epoch % 10 == 0:
      epoch_count.append(epoch)
      loss_values.append(loss)
      test_loss_values.append(test_loss)

      print(f"Epoch: {epoch} | Loss: {loss} | Test Loss: {test_loss}")

      # Print out the model state
      print(Model_0.state_dict())

import numpy as np

epoch_count, np.array(torch.tensor(loss_values).numpy()), test_loss_values

# Plot the loss curves
plt.plot(epoch_count, np.array(torch.tensor(loss_values).numpy()), label = "Training Loss")
plt.plot(epoch_count, test_loss_values, label = "Test Loss")
plt.title("Training and Test Loss Curves")
plt.ylabel("Loss")
plt.xlabel("Epochs")
plt.legend()

with torch.inference_mode():
  y_preds_new = Model_0(X_test)

plot_predictions(predictions = y_preds_new)

weight, bias

"""***Saving a model in Pytorch ***

Three main methods saving and loading model

1. torch.save()
2. torch.load()
3. torch.nn.Module.load_state_dict()
"""

Model_0.state_dict()

# Saving our Pytorch Model

from pathlib import Path

# Create models directory
Model_Path = Path("models")
Model_Path.mkdir(parents=True, exist_ok=True)

# Create model save path
Model_Name = "01_pytorch_workflow_model.pth"
Model_Save_Path = Model_Path / Model_Name

# Save the model state dict
torch.save(obj = Model_0.state_dict(),
           f = Model_Save_Path)

!ls -l models

"""**Loading a Pytorch Model**"""

Model_0.state_dict()

# To load in a saved state_dict we have to instantiate a new instance of our model class

Loaded_Model_0 = LinearRegressionModel()

# Load the saved state_dict of Model_0 (This will update the new instance with updated parameters)

Loaded_Model_0.state_dict(torch.load(f = Model_Save_Path))

Loaded_Model_0.state_dict()

# Make some predictions with our loaded model

Loaded_Model_0.eval()
with torch.inference_mode():
  y_preds_loaded = Loaded_Model_0(X_test)

y_preds == y_preds_loaded

"""# **Putting above all togehter **"""

# import pytorch and matplotlib

import torch
from torch import nn
import matplotlib.pyplot as plt

# Check pytorch version
torch.__version__

"""Create Device Agnostic Code"""

# Setup device agnostic code

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

"""# **Data**"""

# create some data using the linear regression formula of y = weight * X + bias

weight = 0.7
bias = 0.3

start = 0
end = 1
step = 0.02

X = torch.arange(start, end, step).unsqueeze(dim=1)
y = weight * X + bias

X[:10], y[:10]

# split data

train_split = int(0.8 * len(X))
X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test = X[train_split:], y[train_split:]

len(X_train), len(y_train), len(X_test), len(y_test)

# Plot the data
plot_predictions(X_train, y_train, X_test, y_test)

"""# **Building a Pytorch Linear Model**"""

# Create a linear model by subclassing nn.Module

class LinearRegressionModelV2(nn.Module):
  def __init__(self, *args, **kwargs) -> None:
    super().__init__(*args, **kwargs)
    # Use nn.linear() for creating the model parameters / also called: linear transforms
    self.linear_layer = nn.Linear(in_features=1, out_features=1)
  def forward(self, x: torch.Tensor) -> torch.Tensor:
      return self.linear_layer(x)

# Set the manual Seed

torch.manual_seed(42)
Model_1 = LinearRegressionModelV2()
Model_1, Model_1.state_dict()

# Check the model current device

next(Model_1.parameters()).device

# Set the model to use the target device

Model_1.to(device)
next(Model_1.parameters()).device

"""# **Training**

loss func
Optimizer
Training Loop
Testing Loop
"""

# Setup loss function
loss_fn = nn.L1Loss() # same as MAE

# Setup our optimizer
optimizer = torch.optim.SGD(params=Model_1.parameters(), lr=0.01)

torch.manual_seed(42)

epochs = 200

# Put data on the target device (Device Agnostic Code for data)
X_train = X_train.to(device)
y_train = y_train.to(device)
X_test = X_test.to(device)
y_test = y_test.to(device)

for epoch in range(epochs):
  Model_1.train()

  # 1. Forward pass
  y_pred = Model_1(X_train)

  # 2. Calculate the loss
  loss = loss_fn(y_pred, y_train)

  # 3. Optimizer zero grad
  optimizer.zero_grad()

  # 4. Perform backpropagation
  loss.backward()

  # 5. Optimizer step
  optimizer.step()

  ### Testing
  Model_1.eval()
  with torch.inference_mode():
    test_pred = Model_1(X_test)
    test_loss = loss_fn(test_pred, y_test)

  # Print out what's happening
  if epoch % 10 == 0:
    print(f"Epoch: {epoch} | Loss: {loss} | Test Loss: {test_loss}")

Model_1.state_dict()

weight, bias

"""# **Making and evaluatinv predictions**"""

# Turn model into evaluation mode
Model_1.eval()

with torch.inference_mode():
  y_preds = Model_1(X_test)

# Plot predictions
plot_predictions(predictions = y_preds.cpu())

"""# **Saving / Loading a trained model**"""

from pathlib import Path

# Create models directory
Model_Path = Path("models")
Model_Path.mkdir(parents=True, exist_ok=True)

# Create model save path
Model_Name = "01_pytorch_workflow_model_1.pth"
Model_Save_Path = Model_Path / Model_Name

# Save the model state dict
print(f"Saving model to: {Model_Save_Path}")
torch.save(obj = Model_1.state_dict(),
           f = Model_Save_Path)

Model_1.state_dict()

# Load a Pytorch
Loaded_Model_1 = LinearRegressionModelV2()
Loaded_Model_1.load_state_dict(torch.load(f = Model_Save_Path))

# Put the loaded model to device
Loaded_Model_1.to(device)

next(Loaded_Model_1.parameters()).device

Loaded_Model_1.state_dict()

# Evaluate loaded Model
Loaded_Model_1.eval()
with torch.inference_mode():
  y_preds_loaded = Loaded_Model_1(X_test)

# Plot predictions
plot_predictions(predictions = y_preds_loaded.cpu())

y_preds == y_preds_loaded