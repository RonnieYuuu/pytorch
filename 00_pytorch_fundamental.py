# -*- coding: utf-8 -*-
"""00_pytorch_fundamental.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XFtYBA0AmmOBtWvr_F0NdskW5xOGmAt-
"""

!nvidia-smi

import torch
print(torch.__version__)

"""**Creating Tensor**"""

#scalar
scalar = torch.tensor(7)
scalar

scalar.ndim

#Vector
vector = torch.tensor([7,7])
vector

vector.ndim

vector.shape

#Matric
matrix = torch.tensor([[1,2], [9,10]])
matrix

matrix[1]

matrix.shape

#tensor
Tensor = torch.tensor([[[1,2,3],
                        [2,4,5],
                        [1,1,1]]])
Tensor

Tensor.ndim

Tensor.shape

Tensor[0]

"""**Random tensors**"""

# Randon tensors
#Create a random tensor of size (3,4)

random_tensor = torch.rand(3,4)
random_tensor

random_tensor1 = torch.rand(1,10,10)
random_tensor1

print(random_tensor.ndim)
print(random_tensor1.ndim)

random_tensor2 = torch.rand(10,10,10)
random_tensor2

# Create a random tensor with similar shape to an image tensor

random_image_size_tensor = torch.rand(size=(224, 224,3)) #height, width, colour channel
random_image_size_tensor.shape, random_image_size_tensor.ndim

"""**Zeros and Ones**"""

# create a tensor with all zero

zeros = torch.zeros(size = (3,4))
zeros

zeros*random_tensor

# create a tensor with all ones

ones = torch.ones(size = (3,4))
ones

ones.dtype

"""**Creat a tensor range or tensor-like**"""

# using torch.range()

zero_to_nine = torch.arange(0,10)
zero_to_nine

# Creating tensors like

ten_zeros = torch.zeros_like(input = zero_to_nine)
ten_zeros

"""**Tensor Datatype**"""

# Float 32 Tensor

float_32_tensor = torch.tensor([3.0, 6.0, 9.0],
                               dtype=None)
float_32_tensor

float_16_tensor = float_32_tensor.type(torch.float16)
float_16_tensor

float_32_tensor * float_16_tensor

# Get infomation from tensors

# datatype tensor.dtype
# device tensor.device
# shape  tensor.shape

some_tensor = torch.rand(3,4)
some_tensor

"""**Get Info**"""

print(some_tensor)
print(f"Datatype: {some_tensor.dtype}")
print(f"Device: {some_tensor.device}")
print(f"shape: {some_tensor.shape}")

"""**Manipulating tensors (Tensor Operations)**"""

# addition subtraction division multification and matrixs multification

# Create a tensor and add 10 to it
tensor = torch.tensor([1,2,3])
tensor + 10

# multiplication
tensor = tensor * 10
tensor

# try out pytorch built in functions

torch.mul(tensor, 10)

"""**Matrix Multiplication**"""

# Element-Wise Multiplication
print(tensor * tensor)

# One of the most common error in deep learning

torch.matmul(torch.rand(3,2), torch.rand(2,4)) # 3*4

# shapes for matrix multiplication

tensor_A  = torch.tensor([[1,2],
                          [3,4],
                          [5,6]])

tensor_B = torch.tensor([[7,10],
                         [8, 11],
                         [9,12]])


torch.matmul(tensor_A, tensor_B)

tensor_B.T

torch.matmul(tensor_A, tensor_B.T)

torch.matmul(tensor_A, tensor_B.T).shape

# Min Max Mean Sum of tensors

x = torch.arange(0, 100, 10)
x

torch.max(x)

x.max()

x.dtype

x.mean()

torch.mean(x.type(torch.float32))

x.argmin()

"""**Reshape, stacking, squeezing, unsqueezing**"""

x = torch.arange(1., 10.)
x, x.shape

y = torch.arange(1., 13.)
y, y.shape

# reshape Add an extra dimension

x_reshaped = x.reshape(1, 7)

x_reshaped = x.reshape(9, 1)
x_reshaped

y_reshaped = y.reshape(3, 4)
y_reshaped

# Change the view

z = x.view(1, 9)
z, z.shape

# Stack the tensors on the top of each other

x_stacked = torch.stack([x,x,x,x])
x_stacked

x_stacked = torch.stack([x,x,x,x], dim = 1)
x_stacked

x_reshaped.shape
x_reshaped

x_reshaped.shape

x_reshaped.squeeze()

x_reshaped.squeeze().shape #remove extra dimensions

x_reshaped.squeeze().unsqueeze(dim = 0)

x_reshaped.squeeze().unsqueeze(dim = 1)

# torch.permute -- rearrange dimensions of a target tensor

x_original = torch.rand(size = (224, 224, 3))
x_permuted = x_original.permute(2,0,1)
x_permuted

x_original

x_original.shape, x_permuted.shape

x_original[0, 0, 0] = 7777777

x_permuted

# Indexing (selecting data from tensors)

x = torch.arange(1,10).reshape(1,3,3)
x, x.shape

x[0]

x[0][0]

x[0][0][0]

"""**Pytorch, Tensors and Numpy**"""

# Numpy array to tensors

import torch
import numpy as np

array = np.arange(1.0, 8.0)
tensor = torch.from_numpy(array)
array, tensor

# Change the values of array

array = array + 1
array, tensor

# Tensors to numpy array

tensor = torch.ones(7)
numpy_tensor = tensor.numpy()
tensor, numpy_tensor

# Reproducibility (trying to take random out of random)

torch.rand(3,3)

import torch

# create two random tensors

random_tensor_A = torch.rand(3,4)
random_tensor_B = torch.rand(3,4)

print(random_tensor_A)
print(random_tensor_B)
print(random_tensor_A == random_tensor_B)

random_seed = 42

torch.manual_seed(random_seed)
tensor_random_C = torch.rand(3,4)

torch.manual_seed(random_seed)
tensor_random_D = torch.rand(3,4)

print(tensor_random_C)
print(tensor_random_D)
print(tensor_random_C == tensor_random_D)

# Check for GPU access with pytorch
import torch

torch.cuda.is_available()

device = "cuda" if torch.cuda.is_available() else "cpu"
device

"""**Putting Tensors and Models on the GPU**"""

!nvidia-smi

tensor = torch.tensor([1,2,3])

tensor_on_gpu = tensor.to(device)
tensor_on_gpu

# Create a tensor (default on cpu)

tensor_on_gpu.numpy()

tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()
tensor_back_on_cpu